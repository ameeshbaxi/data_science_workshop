{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV into small csv chunk for the sampling purpose\n",
    "\n",
    "#### Data-Set\n",
    "For this assignment purpose, the dataset will be used from [Kaggle Competition](https://www.kaggle.com/c/avazu-ctr-prediction/). Mostlikely, the same challenge will be capstone project as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
      "       'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
      "       'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
      "       'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'],\n",
      "      dtype='object')\n",
      "0    532546046\n",
      "Name: site_id, dtype: int64\n",
      "0    4085536615\n",
      "Name: site_domain, dtype: int64\n",
      "0    680550077\n",
      "Name: site_category, dtype: int64\n",
      "0    3970769798\n",
      "Name: app_id, dtype: int64\n",
      "0    2013391065\n",
      "Name: app_domain, dtype: int64\n",
      "0    131587874\n",
      "Name: app_category, dtype: int64\n",
      "0    2845778250\n",
      "Name: device_id, dtype: int64\n",
      "0    3721564782\n",
      "Name: device_ip, dtype: int64\n",
      "0    1150642724\n",
      "Name: device_model, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/train.csv', nrows=2)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n",
    "columns = ['site_id', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
    "           'device_ip', 'device_model']\n",
    "\n",
    "for c in columns:\n",
    "    df[c] = df[c].apply(lambda x: int(x, 16))\n",
    "    print(df[c].head(1))\n",
    "\n",
    "chunk_count = None\n",
    "chunk_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting csv file into sqllite database\n",
    "Note: The actual function call is commented out so that this function does not get called all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "def process_data_type(df):\n",
    "    df.hour = df.hour.astype(str)\n",
    "    df.hour = df.hour.str[:2] + '-' + df.hour.str[2:]\n",
    "    df.hour = df.hour.str[:5] + '-' + df.hour.str[5:]\n",
    "    df.hour = df.hour.str[:8] + ' ' + df.hour.str[8:]\n",
    "    df.hour = pd.to_datetime(df.hour, format='%y-%m-%d %H')\n",
    "    df.hour = df['hour'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    columns = ['site_id', 'site_domain', 'site_category',\n",
    "               'app_id', 'app_domain', 'app_category',\n",
    "               'device_id', 'device_ip', 'device_model']\n",
    "\n",
    "    for c in columns:\n",
    "        df[c] = df[c].apply(lambda x: int(x, 16))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def divide_csv_into_chuncks(csv_data):\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = chunk_size\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    dir_path = os.path.dirname(os.path.abspath(csv_data))\n",
    "    csv_name = os.path.basename(csv_data).split('.')[0]\n",
    "    for df in pd.read_csv(csv_data, chunksize=chunksize, iterator=True, encoding='utf-8', low_memory=False):\n",
    "        df = process_data_type(df)\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        file_name = os.path.join(dir_path, csv_name + '_chunk%d.csv' % j)\n",
    "        df.to_csv(file_name, index=False)\n",
    "        index_start = df.index[-1] + 1\n",
    "        \n",
    "        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    return j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Database Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#chunk_count = divide_csv_into_chuncks('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 seconds: completed 50000 rows\n",
      "4 seconds: completed 100000 rows\n",
      "7 seconds: completed 150000 rows\n",
      "10 seconds: completed 200000 rows\n",
      "12 seconds: completed 250000 rows\n",
      "15 seconds: completed 300000 rows\n",
      "18 seconds: completed 350000 rows\n",
      "20 seconds: completed 400000 rows\n",
      "23 seconds: completed 450000 rows\n",
      "25 seconds: completed 500000 rows\n",
      "28 seconds: completed 550000 rows\n",
      "31 seconds: completed 600000 rows\n",
      "33 seconds: completed 650000 rows\n",
      "36 seconds: completed 700000 rows\n",
      "38 seconds: completed 750000 rows\n",
      "41 seconds: completed 800000 rows\n",
      "44 seconds: completed 850000 rows\n",
      "47 seconds: completed 900000 rows\n",
      "49 seconds: completed 950000 rows\n",
      "52 seconds: completed 1000000 rows\n",
      "55 seconds: completed 1050000 rows\n",
      "57 seconds: completed 1100000 rows\n",
      "60 seconds: completed 1150000 rows\n",
      "63 seconds: completed 1200000 rows\n",
      "65 seconds: completed 1250000 rows\n",
      "68 seconds: completed 1300000 rows\n",
      "71 seconds: completed 1350000 rows\n",
      "74 seconds: completed 1400000 rows\n",
      "76 seconds: completed 1450000 rows\n",
      "79 seconds: completed 1500000 rows\n",
      "82 seconds: completed 1550000 rows\n",
      "84 seconds: completed 1600000 rows\n",
      "87 seconds: completed 1650000 rows\n",
      "89 seconds: completed 1700000 rows\n",
      "92 seconds: completed 1750000 rows\n",
      "95 seconds: completed 1800000 rows\n",
      "97 seconds: completed 1850000 rows\n",
      "100 seconds: completed 1900000 rows\n",
      "103 seconds: completed 1950000 rows\n",
      "105 seconds: completed 2000000 rows\n",
      "108 seconds: completed 2050000 rows\n",
      "110 seconds: completed 2100000 rows\n",
      "113 seconds: completed 2150000 rows\n",
      "116 seconds: completed 2200000 rows\n",
      "118 seconds: completed 2250000 rows\n",
      "121 seconds: completed 2300000 rows\n",
      "124 seconds: completed 2350000 rows\n",
      "126 seconds: completed 2400000 rows\n",
      "129 seconds: completed 2450000 rows\n",
      "131 seconds: completed 2500000 rows\n",
      "134 seconds: completed 2550000 rows\n",
      "137 seconds: completed 2600000 rows\n",
      "139 seconds: completed 2650000 rows\n",
      "142 seconds: completed 2700000 rows\n",
      "145 seconds: completed 2750000 rows\n",
      "147 seconds: completed 2800000 rows\n",
      "150 seconds: completed 2850000 rows\n",
      "152 seconds: completed 2900000 rows\n",
      "155 seconds: completed 2950000 rows\n",
      "157 seconds: completed 3000000 rows\n",
      "160 seconds: completed 3050000 rows\n",
      "162 seconds: completed 3100000 rows\n",
      "165 seconds: completed 3150000 rows\n",
      "167 seconds: completed 3200000 rows\n",
      "169 seconds: completed 3250000 rows\n",
      "172 seconds: completed 3300000 rows\n",
      "174 seconds: completed 3350000 rows\n",
      "177 seconds: completed 3400000 rows\n",
      "180 seconds: completed 3450000 rows\n",
      "183 seconds: completed 3500000 rows\n",
      "186 seconds: completed 3550000 rows\n",
      "189 seconds: completed 3600000 rows\n",
      "192 seconds: completed 3650000 rows\n",
      "194 seconds: completed 3700000 rows\n",
      "197 seconds: completed 3750000 rows\n",
      "200 seconds: completed 3800000 rows\n",
      "203 seconds: completed 3850000 rows\n",
      "205 seconds: completed 3900000 rows\n",
      "208 seconds: completed 3950000 rows\n",
      "211 seconds: completed 4000000 rows\n",
      "214 seconds: completed 4050000 rows\n",
      "217 seconds: completed 4100000 rows\n",
      "219 seconds: completed 4150000 rows\n",
      "222 seconds: completed 4200000 rows\n",
      "224 seconds: completed 4250000 rows\n",
      "227 seconds: completed 4300000 rows\n",
      "230 seconds: completed 4350000 rows\n",
      "232 seconds: completed 4400000 rows\n",
      "235 seconds: completed 4450000 rows\n",
      "237 seconds: completed 4500000 rows\n",
      "240 seconds: completed 4550000 rows\n",
      "241 seconds: completed 4600000 rows\n"
     ]
    }
   ],
   "source": [
    "test_chunk_count = divide_csv_into_chuncks('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chunk_count"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
